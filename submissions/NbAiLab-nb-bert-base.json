{
    "repo_link": "https://github.com/JoeyOhman/SuperLim-2-Testing",
    "model": {
        "model_name": "NbAiLab/nb-bert-base",
        "model_type": "Encoder",
        "number_params": 177853440,
        "data_size": 109
    },
    "tasks": {
        "DaLAJ": {
            "dev": {
                "f1": null,
                "alpha": 0.6530319741313557
            },
            "test": {
                "f1": null,
                "alpha": 0.6622599102581453
            }
        },
        "Swedish FAQ": {
            "dev": {
                "f1": null,
                "alpha": 0.7073235923022094
            },
            "test": {
                "f1": null,
                "alpha": 0.6802696789748628
            }
        },
        "SweMNLI": {
            "dev": {
                "f1": null,
                "alpha": 0.7174074679927003
            },
            "test": {
                "f1": null,
                "alpha": 0.17158263524622241
            }
        },
        "SweWinograd": {
            "dev": {
                "f1": null,
                "alpha": 0.17708513708513707
            },
            "test": {
                "f1": null,
                "alpha": 0.12036063941589692
            }
        },
        "SweWinogender": {
            "dev": {
                "f1": null,
                "parity": null,
                "alpha": null
            },
            "test": {
                "f1": null,
                "parity": 0.9903846153846154,
                "alpha": -0.3323056259348576
            }
        },
        "SweDiagnostics": {
            "dev": {
                "f1": null,
                "alpha": null
            },
            "test": {
                "f1": null,
                "alpha": null
            }
        },
        "SweParaphrase": {
            "dev": {
                "f1": null,
                "alpha": 0.8797653280972183
            },
            "test": {
                "f1": null,
                "alpha": 0.8226824891263553
            }
        },
        "SweWiC": {
            "dev": {
                "f1": null,
                "alpha": 0.35161057692307696
            },
            "test": {
                "f1": null,
                "alpha": 0.31754466293939976
            }
        },
        "SweSAT": {
            "dev": {
                "f1": null,
                "alpha": null
            },
            "test": {
                "f1": null,
                "alpha": null
            }
        },
        "Swedish Analogy": {
            "dev": {
                "f1": null,
                "alpha": null
            },
            "test": {
                "f1": null,
                "alpha": null
            }
        },
        "SuperSim": {
            "dev": {
                "f1": null,
                "alpha": null
            },
            "test": {
                "f1": null,
                "alpha": null
            }
        },
        "ABSA": {
            "dev": {
                "f1": null,
                "alpha": 0.4563623368183589
            },
            "test": {
                "f1": null,
                "alpha": 0.39288483040537536
            }
        }
    },
    "model_card": "---\nlanguage: no\nlicense: cc-by-4.0\ntags:\n- norwegian\n- bert\npipeline_tag: fill-mask\nwidget:\n- text: P\u00e5 biblioteket kan du [MASK] en bok.\n- text: Dette er et [MASK] eksempel.\n- text: Av og til kan en spr\u00e5kmodell gi et [MASK] resultat.\n- text: Som ansat f\u00e5r du [MASK] for at bidrage til borgernes adgang til dansk kulturarv, til forskning og til samfundets demokratiske udvikling.\n---\n- **Release 1.1** (March 11, 2021)\n- **Release 1.0** (January 13, 2021)\n\n# NB-BERT-base\n\n## Description\n\nNB-BERT-base is a general BERT-base model built on the large digital collection at the National Library of Norway.\n\nThis model is based on the same structure as [BERT Cased multilingual model](https://github.com/google-research/bert/blob/master/multilingual.md), and is trained on a wide variety of Norwegian text (both bokm\u00e5l and nynorsk) from the last 200 years.\n\n## Intended use & limitations\n\nThe 1.1 version of the model is general, and should be fine-tuned for any particular use. Some fine-tuning sets may be found on GitHub, see\n\n* https://github.com/NBAiLab/notram\n\n## Training data\n\nThe model is trained on a wide variety of text. The training set is described on\n\n* https://github.com/NBAiLab/notram\n\n## More information\n\nFor more information on the model, see\n\nhttps://github.com/NBAiLab/notram"
}