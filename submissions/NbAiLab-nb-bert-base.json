{
    "repo_link": "https://github.com/JoeyOhman/SuperLim-2-Testing",
    "model": {
        "model_name": "NbAiLab/nb-bert-base",
        "model_type": "Encoder",
        "number_params": 177853440,
        "data_size": 109
    },
    "tasks": {
        "absabank-imm": {
            "dev": {
                "other": null,
                "alpha": 0.456362
            },
            "test": {
                "other": null,
                "alpha": 0.392885
            }
        },
        "argumentation_sentences": {
            "dev": {
                "other": null,
                "alpha": null
            },
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "dalaj-ged-superlim": {
            "dev": {
                "other": null,
                "alpha": 0.638692
            },
            "test": {
                "other": null,
                "alpha": 0.64446
            }
        },
        "supersim-superlim-relatedness": {
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "supersim-superlim-similarity": {
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "sweanalogy": {
            "test": {
                "other": null,
                "accuracy": null
            }
        },
        "swediagnostics": {
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "swedn": {
            "dev": {
                "other": null,
                "f1": null
            },
            "test": {
                "other": null,
                "f1": null
            }
        },
        "swefaq": {
            "dev": {
                "other": null,
                "alpha": 0.707324
            },
            "test": {
                "other": null,
                "alpha": 0.68027
            }
        },
        "swemnli": {
            "dev": {
                "other": null,
                "alpha": 0.717407
            },
            "test": {
                "other": null,
                "alpha": 0.171583
            }
        },
        "sweparaphrase": {
            "dev": {
                "other": null,
                "alpha": 0.879765
            },
            "test": {
                "other": null,
                "alpha": 0.822682
            }
        },
        "swesat-synonyms": {
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "swewic": {
            "dev": {
                "other": null,
                "alpha": 0.351611
            },
            "test": {
                "other": null,
                "alpha": 0.317545
            }
        },
        "swewinogender": {
            "test": {
                "other": null,
                "alpha": -0.332306,
                "parity": 0.990385
            }
        },
        "swewinograd": {
            "dev": {
                "other": null,
                "alpha": 0.177085
            },
            "test": {
                "other": null,
                "alpha": 0.120361
            }
        }
    },
    "model_card": "---\nlanguage: no\nlicense: cc-by-4.0\ntags:\n- norwegian\n- bert\npipeline_tag: fill-mask\nwidget:\n- text: P\u00e5 biblioteket kan du [MASK] en bok.\n- text: Dette er et [MASK] eksempel.\n- text: Av og til kan en spr\u00e5kmodell gi et [MASK] resultat.\n- text: Som ansat f\u00e5r du [MASK] for at bidrage til borgernes adgang til dansk kulturarv, til forskning og til samfundets demokratiske udvikling.\n---\n- **Release 1.1** (March 11, 2021)\n- **Release 1.0** (January 13, 2021)\n\n# NB-BERT-base\n\n## Description\n\nNB-BERT-base is a general BERT-base model built on the large digital collection at the National Library of Norway.\n\nThis model is based on the same structure as [BERT Cased multilingual model](https://github.com/google-research/bert/blob/master/multilingual.md), and is trained on a wide variety of Norwegian text (both bokm\u00e5l and nynorsk) from the last 200 years.\n\n## Intended use & limitations\n\nThe 1.1 version of the model is general, and should be fine-tuned for any particular use. Some fine-tuning sets may be found on GitHub, see\n\n* https://github.com/NBAiLab/notram\n\n## Training data\n\nThe model is trained on a wide variety of text. The training set is described on\n\n* https://github.com/NBAiLab/notram\n\n## More information\n\nFor more information on the model, see\n\nhttps://github.com/NBAiLab/notram"
}