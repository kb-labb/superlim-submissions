{
    "repo_link": "https://github.com/JoeyOhman/SuperLim-2-Testing",
    "model": {
        "model_name": "KB/bert-base-swedish-cased",
        "model_type": "Encoder",
        "number_params": 124690944,
        "data_size": 20
    },
    "tasks": {
        "absabank-imm": {
            "dev": {
                "N": 487,
                "alpha": 0.513307018161707,
                "spearmanr": 0.5088997453248235,
                "pearsonr": 0.5322007523512216
            },
            "test": {
                "N": 487,
                "alpha": 0.5291896524571031,
                "spearmanr": 0.5731187741292629,
                "pearsonr": 0.5543960437745663
            }
        },
        "argumentation-sentences": {
            "dev": {
                "N": 750,
                "alpha": 0.9298156224480943,
                "accuracy": 0.9546666666666667,
                "f1": 0.9517991837408472
            },
            "test": {
                "N": 1065,
                "alpha": 0.5550275537634408,
                "accuracy": 0.707981220657277,
                "f1": 0.7014739639966171
            }
        },
        "dalaj-ged-superlim": {
            "dev": {
                "N": 4702,
                "alpha": 0.7159831472978004,
                "accuracy": 0.8592088472990217,
                "f1": 0.8579764711894351
            },
            "test": {
                "N": 4371,
                "alpha": 0.7397153107439985,
                "accuracy": 0.8705101807366735,
                "f1": 0.8698427666470676
            }
        },
        "supersim-superlim-relatedness": {
            "test": {
                "N": null,
                "alpha": null,
                "spearmanr": null,
                "pearsonr": null
            }
        },
        "supersim-superlim-similarity": {
            "test": {
                "N": null,
                "alpha": null,
                "spearmanr": null,
                "pearsonr": null
            }
        },
        "sweanalogy": {
            "test": {
                "N": null,
                "accuracy": null,
                "pseudoalpha": null
            }
        },
        "swediagnostics": {
            "test": {
                "N": null,
                "alpha": null,
                "accuracy": null,
                "f1": null
            }
        },
        "swedn": {
            "dev": {
                "rouge1": null,
                "rouge2": null,
                "rougeL": null,
                "bleu": null,
                "translation_length": null,
                "reference_length": null,
                "accuracy": null
            },
            "test": {
                "rouge1": null,
                "rouge2": null,
                "rougeL": null,
                "bleu": null,
                "translation_length": null,
                "reference_length": null,
                "accuracy": null
            }
        },
        "swefaq": {
            "dev": {
                "N": 110,
                "accuracy": 0.7636363636363637,
                "pseudoalpha": 0.7503561387066542
            },
            "test": {
                "N": 109,
                "accuracy": 0.6605504587155964,
                "pseudoalpha": 0.6414782937671427
            }
        },
        "swenli": {
            "dev": {
                "N": 9815,
                "alpha": 0.720911278605919,
                "accuracy": 0.8141619969434539,
                "f1": 0.8128364279539994
            },
            "test": {
                "N": 305,
                "alpha": 0.17911563694569355,
                "accuracy": 0.580327868852459,
                "f1": 0.45609950430313534
            }
        },
        "sweparaphrase": {
            "dev": {
                "N": 1499,
                "alpha": 0.9005273530403447,
                "spearmanr": 0.9014469121316716,
                "pearsonr": 0.9038389260503722
            },
            "test": {
                "N": 1378,
                "alpha": 0.8448653664597761,
                "spearmanr": 0.8428959464835868,
                "pearsonr": 0.8542478680084438
            }
        },
        "swesat-synonyms": {
            "test": {
                "N": null,
                "accuracy": null,
                "pseudoalpha": null
            }
        },
        "swewic": {
            "dev": {
                "N": 500,
                "alpha": 0.40850682498279745,
                "accuracy": 0.704,
                "f1": 0.70395736986126
            },
            "test": {
                "N": 1000,
                "alpha": 0.37618973318770477,
                "accuracy": 0.688,
                "f1": 0.6879388360118583
            }
        },
        "swewinogender": {
            "test": {
                "N": 624,
                "alpha": -0.3322649572649572,
                "parity": 0.3333333333333333,
                "accuracy": 0.5,
                "f1": 0.3333333333333333
            }
        },
        "swewinograd": {
            "dev": {
                "N": 135,
                "alpha": 0.18820301783264748,
                "accuracy": 0.5925925925925926,
                "f1": 0.5925925925925926
            },
            "test": {
                "N": 140,
                "alpha": 0.1394580304031725,
                "accuracy": 0.6,
                "f1": 0.5681868252919146
            }
        }
    },
    "model_card": "---\nlanguage: sv\n---\n\n# Swedish BERT Models\n\nThe National Library of Sweden / KBLab releases three pretrained language models based on BERT and ALBERT. The models are trained on aproximately 15-20GB of text (200M sentences, 3000M tokens) from various sources (books, news, government publications, swedish wikipedia and internet forums) aiming to provide a representative BERT model for Swedish text. A more complete description will be published later on.\n\nThe following three models are currently available:\n\n- **bert-base-swedish-cased** (*v1*) - A BERT trained with the same hyperparameters as first published by Google.\n- **bert-base-swedish-cased-ner** (*experimental*) - a BERT fine-tuned for NER using SUC 3.0.\n- **albert-base-swedish-cased-alpha** (*alpha*) - A first attempt at an ALBERT for Swedish.\n\nAll models are cased and trained with whole word masking.\n\n## Files\n\n| **name**                        | **files** |\n|---------------------------------|-----------|\n| bert-base-swedish-cased         | [config](https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased/config.json), [vocab](https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased/vocab.txt), [pytorch_model.bin](https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased/pytorch_model.bin) |\n| bert-base-swedish-cased-ner     | [config](https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased-ner/config.json), [vocab](https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased-ner/vocab.txt) [pytorch_model.bin](https://s3.amazonaws.com/models.huggingface.co/bert/KB/bert-base-swedish-cased-ner/pytorch_model.bin) |\n| albert-base-swedish-cased-alpha | [config](https://s3.amazonaws.com/models.huggingface.co/bert/KB/albert-base-swedish-cased-alpha/config.json), [sentencepiece model](https://s3.amazonaws.com/models.huggingface.co/bert/KB/albert-base-swedish-cased-alpha/spiece.model), [pytorch_model.bin](https://s3.amazonaws.com/models.huggingface.co/bert/KB/albert-base-swedish-cased-alpha/pytorch_model.bin) |\n\nTensorFlow model weights will be released soon.\n\n## Usage requirements / installation instructions\n\nThe examples below require Huggingface Transformers 2.4.1 and Pytorch 1.3.1 or greater. For Transformers<2.4.0 the tokenizer must be instantiated manually and the `do_lower_case` flag parameter set to `False` and `keep_accents` to `True` (for ALBERT).\n\nTo create an environment where the examples can be run, run the following in an terminal on your OS of choice.\n\n```\n# git clone https://github.com/Kungbib/swedish-bert-models\n# cd swedish-bert-models\n# python3 -m venv venv\n# source venv/bin/activate\n# pip install --upgrade pip\n# pip install -r requirements.txt\n```\n\n### BERT Base Swedish\n\nA standard BERT base for Swedish trained on a variety of sources. Vocabulary size is ~50k. Using Huggingface Transformers the model can be loaded in Python as follows:\n\n```python\nfrom transformers import AutoModel,AutoTokenizer\n\ntok = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\nmodel = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\n```\n\n\n### BERT base fine-tuned for Swedish NER\n\nThis model is fine-tuned on the SUC 3.0 dataset. Using the Huggingface pipeline the model can be easily instantiated. For Transformer<2.4.1 it seems the tokenizer must be loaded separately to disable lower-casing of input strings:\n\n```python\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model='KB/bert-base-swedish-cased-ner', tokenizer='KB/bert-base-swedish-cased-ner')\n\nnlp('Idag sl\u00e4pper KB tre spr\u00e5kmodeller.')\n```\n\nRunning the Python code above should produce in something like the result below. Entity types used are `TME` for time, `PRS` for personal names, `LOC` for locations, `EVN` for events and `ORG` for organisations. These labels are subject to change.\n\n```python\n[ { 'word': 'Idag', 'score': 0.9998126029968262, 'entity': 'TME' },\n  { 'word': 'KB',   'score': 0.9814832210540771, 'entity': 'ORG' } ]\n```\n\nThe BERT tokenizer often splits words into multiple tokens, with the subparts starting with `##`, for example the string `Engelbert k\u00f6r Volvo till Herr\u00e4ngens fotbollsklubb` gets tokenized as `Engel ##bert k\u00f6r Volvo till Herr ##\u00e4ngens fotbolls ##klubb`. To glue parts back together one can use something like this:\n\n```python\ntext = 'Engelbert tar Volvon till Tele2 Arena f\u00f6r att titta p\u00e5 Djurg\u00e5rden IF ' +\\\n       'som spelar fotboll i VM klockan tv\u00e5 p\u00e5 kv\u00e4llen.'\n\nl = []\nfor token in nlp(text):\n    if token['word'].startswith('##'):\n        l[-1]['word'] += token['word'][2:]\n    else:\n        l += [ token ]\n\nprint(l)\n```\n\nWhich should result in the following (though less cleanly formated):\n\n```python\n[ { 'word': 'Engelbert',     'score': 0.99..., 'entity': 'PRS'},\n  { 'word': 'Volvon',        'score': 0.99..., 'entity': 'OBJ'},\n  { 'word': 'Tele2',         'score': 0.99..., 'entity': 'LOC'},\n  { 'word': 'Arena',         'score': 0.99..., 'entity': 'LOC'},\n  { 'word': 'Djurg\u00e5rden',    'score': 0.99..., 'entity': 'ORG'},\n  { 'word': 'IF',            'score': 0.99..., 'entity': 'ORG'},\n  { 'word': 'VM',            'score': 0.99..., 'entity': 'EVN'},\n  { 'word': 'klockan',       'score': 0.99..., 'entity': 'TME'},\n  { 'word': 'tv\u00e5',           'score': 0.99..., 'entity': 'TME'},\n  { 'word': 'p\u00e5',            'score': 0.99..., 'entity': 'TME'},\n  { 'word': 'kv\u00e4llen',       'score': 0.54..., 'entity': 'TME'} ]\n```\n\n### ALBERT base\n\nThe easisest way to do this is, again, using Huggingface Transformers:\n\n```python\nfrom transformers import AutoModel,AutoTokenizer\n\ntok = AutoTokenizer.from_pretrained('KB/albert-base-swedish-cased-alpha'),\nmodel = AutoModel.from_pretrained('KB/albert-base-swedish-cased-alpha')\n```\n\n## Acknowledgements \u2764\ufe0f\n\n- Resources from Stockholms University, Ume\u00e5 University and Swedish Language Bank at Gothenburg University were used when fine-tuning BERT for NER.\n- Model pretraining was made partly in-house at the KBLab and partly (for material without active copyright) with the support of Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).\n- Models are hosted on S3 by Huggingface \ud83e\udd17\n"
}