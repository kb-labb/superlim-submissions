{
    "repo_link": "https://github.com/JoeyOhman/SuperLim-2-Testing",
    "model": {
        "model_name": "AI-Nordics/bert-large-swedish-cased",
        "model_type": "Encoder",
        "number_params": 335215616,
        "data_size": 85
    },
    "tasks": {
        "absabank-imm": {
            "dev": {
                "other": null,
                "alpha": 0.534861
            },
            "test": {
                "other": null,
                "alpha": 0.481386
            }
        },
        "argumentation_sentences": {
            "dev": {
                "other": null,
                "alpha": null
            },
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "dalaj-ged-superlim": {
            "dev": {
                "other": null,
                "alpha": 0.716007
            },
            "test": {
                "other": null,
                "alpha": 0.745449
            }
        },
        "supersim-superlim-relatedness": {
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "supersim-superlim-similarity": {
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "sweanalogy": {
            "test": {
                "other": null,
                "accuracy": null
            }
        },
        "swediagnostics": {
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "swedn": {
            "dev": {
                "other": null,
                "f1": null
            },
            "test": {
                "other": null,
                "f1": null
            }
        },
        "swefaq": {
            "dev": {
                "other": null,
                "alpha": 0.864126
            },
            "test": {
                "other": null,
                "alpha": 0.709803
            }
        },
        "swemnli": {
            "dev": {
                "other": null,
                "alpha": 0.746819
            },
            "test": {
                "other": null,
                "alpha": 0.240594
            }
        },
        "sweparaphrase": {
            "dev": {
                "other": null,
                "alpha": 0.895624
            },
            "test": {
                "other": null,
                "alpha": 0.862353
            }
        },
        "swesat-synonyms": {
            "test": {
                "other": null,
                "alpha": null
            }
        },
        "swewic": {
            "dev": {
                "other": null,
                "alpha": 0.285408
            },
            "test": {
                "other": null,
                "alpha": 0.316298
            }
        },
        "swewinogender": {
            "test": {
                "other": null,
                "alpha": -0.332265,
                "parity": 1.0
            }
        },
        "swewinograd": {
            "dev": {
                "other": null,
                "alpha": 0.240471
            },
            "test": {
                "other": null,
                "alpha": 0.191522
            }
        }
    },
    "model_card": "---\nlanguage: sv\n---\n\n# A Swedish Bert model\n\n## Model description\nThis model follows the Bert Large model architecture as implemented in [Megatron-LM framework](https://github.com/NVIDIA/Megatron-LM). It was trained with a batch size of 512 in 600k steps. The model contains following parameters:\n<figure>\n\n| Hyperparameter       | Value      |\n|----------------------|------------|\n| \\\\(n_{parameters}\\\\) | 340M |\n| \\\\(n_{layers}\\\\)     | 24    |\n| \\\\(n_{heads}\\\\)      | 16         |\n| \\\\(n_{ctx}\\\\)        | 1024       |\n| \\\\(n_{vocab}\\\\)        | 30592       |\n\n\n## Training data\nThe model is pretrained on a Swedish text corpus of around 85 GB from a variety of sources as shown below.\n<figure>\n\n| Dataset       | Genre      | Size(GB)|\n|----------------------|------|------|\n| Anf\u00f6randen      | Politics  |0.9|\n|DCEP|Politics|0.6|\n|DGT|Politics|0.7|\n|Fass|Medical|0.6|\n|F\u00f6rfattningar|Legal|0.1|\n|Web data|Misc|45.0|\n|JRC|Legal|0.4|\n|Litteraturbanken|Books|0.3O|\n|SCAR|Misc|28.0|\n|SOU|Politics|5.3|\n|Subtitles|Drama|1.3|\n|Wikipedia|Facts|1.8|\n\n\n## Intended uses & limitations\nThe raw model can be used for the usual tasks of masked language modeling or next sentence prediction. It is also often fine-tuned on a downstream task to improve its performance in a specific domain/task.\n<br>\n<br>\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\"AI-Nordics/bert-large-swedish-cased\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"AI-Nordics/bert-large-swedish-cased\")\n"
}